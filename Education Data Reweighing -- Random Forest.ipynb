{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7ed3933-288b-477b-86a2-da940688e18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-09 23:05:27.378702: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# Load all necessary packages\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from aif360.datasets import CompasDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
    "\n",
    "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions\\\n",
    "                import load_preproc_data_adult, load_preproc_data_compas\n",
    "\n",
    "from aif360.algorithms.postprocessing import EqOddsPostprocessing\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9a396cf-81aa-4562-8d4b-2787114da0b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "               instance weights features                                   \\\n",
       "                                         protected attribute                \n",
       "                                  school                 sex  age address   \n",
       "instance names                                                              \n",
       "0                           1.0      0.0                 1.0  3.0     0.0   \n",
       "1                           1.0      0.0                 1.0  2.0     0.0   \n",
       "2                           1.0      0.0                 1.0  0.0     0.0   \n",
       "3                           1.0      0.0                 1.0  0.0     0.0   \n",
       "4                           1.0      0.0                 1.0  1.0     0.0   \n",
       "...                         ...      ...                 ...  ...     ...   \n",
       "390                         1.0      1.0                 0.0  5.0     0.0   \n",
       "391                         1.0      1.0                 0.0  2.0     0.0   \n",
       "392                         1.0      1.0                 0.0  6.0     1.0   \n",
       "393                         1.0      1.0                 0.0  3.0     1.0   \n",
       "394                         1.0      1.0                 0.0  4.0     0.0   \n",
       "\n",
       "                                               ...                             \\\n",
       "                                               ...                              \n",
       "               famsize Pstatus Medu Fedu Mjob  ... famrel freetime goout Dalc   \n",
       "instance names                                 ...                              \n",
       "0                  0.0     1.0  4.0  4.0  3.0  ...    4.0      3.0   4.0  1.0   \n",
       "1                  0.0     0.0  1.0  1.0  3.0  ...    5.0      3.0   3.0  1.0   \n",
       "2                  0.0     0.0  1.0  1.0  3.0  ...    4.0      3.0   2.0  2.0   \n",
       "3                  0.0     0.0  4.0  2.0  1.0  ...    3.0      2.0   2.0  1.0   \n",
       "4                  0.0     0.0  3.0  3.0  4.0  ...    4.0      3.0   2.0  1.0   \n",
       "...                ...     ...  ...  ...  ...  ...    ...      ...   ...  ...   \n",
       "390                0.0     1.0  2.0  2.0  2.0  ...    5.0      5.0   4.0  4.0   \n",
       "391                0.0     0.0  3.0  1.0  2.0  ...    2.0      4.0   5.0  3.0   \n",
       "392                0.0     0.0  1.0  1.0  4.0  ...    5.0      5.0   3.0  3.0   \n",
       "393                0.0     0.0  3.0  2.0  2.0  ...    4.0      4.0   1.0  3.0   \n",
       "394                0.0     0.0  1.0  1.0  4.0  ...    3.0      2.0   3.0  3.0   \n",
       "\n",
       "                                                labels  \n",
       "                                                        \n",
       "               Walc health absences    G1    G2         \n",
       "instance names                                          \n",
       "0               1.0    3.0      6.0   5.0   6.0    0.0  \n",
       "1               1.0    3.0      4.0   5.0   5.0    0.0  \n",
       "2               3.0    3.0     10.0   7.0   8.0    0.0  \n",
       "3               1.0    5.0      2.0  15.0  14.0    1.0  \n",
       "4               2.0    5.0      4.0   6.0  10.0    0.0  \n",
       "...             ...    ...      ...   ...   ...    ...  \n",
       "390             5.0    4.0     11.0   9.0   9.0    0.0  \n",
       "391             4.0    2.0      3.0  14.0  16.0    1.0  \n",
       "392             3.0    3.0      3.0  10.0   8.0    0.0  \n",
       "393             4.0    5.0      0.0  11.0  12.0    0.0  \n",
       "394             3.0    5.0      5.0   8.0   9.0    0.0  \n",
       "\n",
       "[395 rows x 34 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## import dataset\n",
    "import csv\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "dataset_used = \"/Users/a/Downloads/student+performance/student/student-mat.csv\"\n",
    "headers = \"school;sex;age;address;famsize;Pstatus;Medu;Fedu;Mjob;Fjob;reason;guardian;traveltime;studytime;failures;schoolsup;famsup;paid;activities;nursery;higher;internet;romantic;famrel;freetime;goout;Dalc;Walc;health;absences;G1;G2;G3\"\n",
    "separated_headers = headers.split(';')\n",
    "df = pd.read_csv(dataset_used, delimiter=\";\")\n",
    "d_g3 = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0, 10:0, 11:1, 12:1, 13:1, 14:1, 15:1, 16:1, 17:1, 18:1, 19:1, 20:1} #convert into a binary outcome\n",
    "df['G3'] = df['G3'].map(d_g3)\n",
    "df['labels'] = df['G3']\n",
    "df = df.drop('G3', axis = 'columns')\n",
    " \n",
    "#convert categorical to numeric\n",
    "d_school = {'GP': 0, 'MS': 1}\n",
    "d_sex = {'M': 0, 'F': 1}\n",
    "d_age = {15:0, 16:1, 17:2, 18:3, 19:4, 20:5, 21:6, 22:7}\n",
    "d_address = {'U': 0, 'R': 1}\n",
    "d_famsize = {'LE3': 0, 'GT3': 0}\n",
    "d_pstatus = {'T': 0, 'A': 1}\n",
    " \n",
    "d_mjob = {'teacher': 0, 'health': 1, 'services': 2, 'at_home': 3, 'other': 4}\n",
    "d_fjob = {'teacher': 0, 'health': 1, 'services': 2, 'at_home': 3, 'other': 4}\n",
    "d_reason = {'home': 0, 'reputation': 1, 'course': 2, 'other': 3}\n",
    "d_guardian = {'mother': 0, 'father': 1, 'other': 2} \n",
    "\n",
    "d_schoolsup = {'yes': 0, 'no': 1}\n",
    "d_famsup = {'yes': 0, 'no': 1}\n",
    "d_paid = {'yes': 0, 'no': 1}\n",
    "d_activities = {'yes': 0, 'no': 1}\n",
    "d_nursery = {'yes': 0, 'no': 1}\n",
    "d_higher = {'yes': 0, 'no': 1}\n",
    "d_internet = {'yes': 0, 'no': 1}\n",
    "d_romantic = {'yes': 0, 'no': 1}\n",
    " \n",
    "#apply mappings\n",
    "df['school'] = df['school'].map(d_school)\n",
    "df['sex'] = df['sex'].map(d_sex)\n",
    "df['age'] = df['age'].map(d_age)\n",
    "df['address'] = df['address'].map(d_address)\n",
    "df['famsize'] = df['famsize'].map(d_famsize)\n",
    "df['Pstatus'] = df['Pstatus'].map(d_pstatus)\n",
    " \n",
    "df['Mjob'] = df['Mjob'].map(d_mjob)\n",
    "df['Fjob'] = df['Fjob'].map(d_fjob)\n",
    "df['reason'] = df['reason'].map(d_reason)\n",
    "df['guardian'] = df['guardian'].map(d_guardian)\n",
    " \n",
    "df['schoolsup'] = df['schoolsup'].map(d_paid)\n",
    "df['famsup'] = df['famsup'].map(d_paid)\n",
    "df['paid'] = df['paid'].map(d_paid)\n",
    "df['activities'] = df['activities'].map(d_paid)\n",
    "df['nursery'] = df['nursery'].map(d_paid)\n",
    "df['higher'] = df['higher'].map(d_paid)\n",
    "df['internet'] = df['internet'].map(d_paid)\n",
    "df['romantic'] = df['romantic'].map(d_paid)\n",
    " \n",
    "df = df.dropna()\n",
    "df = df.apply(pd.to_numeric)\n",
    "df.head()\n",
    "\n",
    "new_data = BinaryLabelDataset(favorable_label=1, unfavorable_label=0, df=df, label_names = [\"labels\"], protected_attribute_names=[\"sex\",\"romantic\"])\n",
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5a90853-846f-4594-b64b-1c6b11981bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "dataset_orig = new_data\n",
    "dataset_orig_train, dataset_orig_vt = dataset_orig.split([0.6], shuffle=True)\n",
    "dataset_orig_valid, dataset_orig_test = dataset_orig_vt.split([0.5], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9b484a1-55bd-424c-80ba-c068aae38c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Dataset shape"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(237, 32)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Favorable and unfavorable labels"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Protected attribute names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sex', 'romantic']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Privileged and unprivileged protected attribute values"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.]), array([1.])] [array([0.]), array([0.])]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Dataset feature names"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2']\n"
     ]
    }
   ],
   "source": [
    "# print out some labels, names, etc.\n",
    "display(Markdown(\"#### Dataset shape\"))\n",
    "# This line uses the display function from IPython's display module and Markdown function to format the string \n",
    "#\"#### Dataset shape\" as a markdown header. This will show up as a heading in the output cell of a Jupyter notebook.\n",
    "print(dataset_orig_train.features.shape)\n",
    "#The shape is a tuple that indicates the number of instances and features in the dataset (rows, columns).\n",
    "display(Markdown(\"#### Favorable and unfavorable labels\"))\n",
    "print(dataset_orig_train.favorable_label, dataset_orig_train.unfavorable_label)\n",
    "#In the context of fairness, favorable labels might denote a positive outcome (like being granted bail),\n",
    "#while unfavorable labels might denote a negative outcome (like being denied bail)\n",
    "display(Markdown(\"#### Protected attribute names\"))\n",
    "print(dataset_orig_train.protected_attribute_names)\n",
    "#This prints the names of the protected attributes. Protected attributes are typically those that \n",
    "#should not be used for decision-making due to fairness considerations (e.g., race, gender).\n",
    "display(Markdown(\"#### Privileged and unprivileged protected attribute values\"))\n",
    "print(dataset_orig_train.privileged_protected_attributes, dataset_orig_train.unprivileged_protected_attributes)\n",
    "#This prints the values within the protected attributes that are considered privileged and unprivileged. \n",
    "#Privileged values are those that traditionally benefit from societal bias (e.g., being male in a gender attribute), \n",
    "#while unprivileged values are those that traditionally do not (e.g., being female in a gender attribute).\n",
    "display(Markdown(\"#### Dataset feature names\"))\n",
    "print(dataset_orig_train.feature_names) #features that are used to train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "750d595f-a746-4f32-9753-46f3f74b25aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "unprivileged_groups = [{\"sex\" : 0}]\n",
    "privileged_groups = [{\"sex\" : 1}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "254ae22a-08c5-4d54-9845-389d56082c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Original training dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = 0.038889\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Original validation dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = 0.162338\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Original test dataset"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = 0.077273\n"
     ]
    }
   ],
   "source": [
    "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
    "                             unprivileged_groups=unprivileged_groups,\n",
    "                             privileged_groups=privileged_groups)\n",
    "#BinaryLabelDatasetMetric is a class that provides metrics to evaluate how different the outcomes are across groups defined by protected attributes. \n",
    "#It is being used here to calculate the mean difference in outcomes.\n",
    "display(Markdown(\"#### Original training dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())\n",
    "\n",
    "metric_orig_valid = BinaryLabelDatasetMetric(dataset_orig_valid, \n",
    "                             unprivileged_groups=unprivileged_groups,\n",
    "                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Original validation dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_valid.mean_difference())\n",
    "\n",
    "metric_orig_test = BinaryLabelDatasetMetric(dataset_orig_test, \n",
    "                             unprivileged_groups=unprivileged_groups,\n",
    "                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Original test dataset\"))\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_test.mean_difference())\n",
    "# the unprivileged group received less favorable outcomes compared to the privileged group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fd774f6-d88e-4bd0-bdca-9c757853e5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "#Same as Logistic\n",
    "dataset_orig_train_pred = dataset_orig_train.copy(deepcopy=True)\n",
    "dataset_orig_valid_pred = dataset_orig_valid.copy(deepcopy=True)\n",
    "dataset_orig_test_pred = dataset_orig_test.copy(deepcopy=True)\n",
    "#standardscaler\n",
    "scale_orig = StandardScaler()\n",
    "X_train = scale_orig.fit_transform(dataset_orig_train.features)\n",
    "y_train = dataset_orig_train.labels.ravel()\n",
    "#the random forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "#get index of the favorable class in the dataset\n",
    "fav_idx = np.where(rf_model.classes_ == dataset_orig_train.favorable_label)[0][0]\n",
    "#predict probabilities for the training, validation, and testing sets\n",
    "y_train_pred_prob = rf_model.predict_proba(X_train)[:, fav_idx]\n",
    "\n",
    "X_valid = scale_orig.transform(dataset_orig_valid.features)\n",
    "y_valid_pred_prob = rf_model.predict_proba(X_valid)[:, fav_idx]\n",
    "\n",
    "X_test = scale_orig.transform(dataset_orig_test.features)\n",
    "y_test_pred_prob = rf_model.predict_proba(X_test)[:, fav_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e24b3189-185b-4720-bb18-0570c36ab2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Original-Predicted training dataset\n",
      "Difference in GFPR between unprivileged and privileged groups\n",
      "0.016067639257294444\n",
      "Difference in GFNR between unprivileged and privileged groups\n",
      "-0.014610421836228273\n",
      "#### Original-Predicted validation dataset\n",
      "Difference in GFPR between unprivileged and privileged groups\n",
      "-0.020769230769230734\n",
      "Difference in GFNR between unprivileged and privileged groups\n",
      "0.016555555555555573\n",
      "#### Original-Predicted testing dataset\n",
      "Difference in GFPR between unprivileged and privileged groups\n",
      "-0.03285714285714286\n",
      "Difference in GFNR between unprivileged and privileged groups\n",
      "-0.0552173913043478\n"
     ]
    }
   ],
   "source": [
    "class_thresh = 0.5\n",
    "\n",
    "# Update prediction score\n",
    "dataset_orig_train_pred.scores = y_train_pred_prob.reshape(-1, 1)\n",
    "dataset_orig_valid_pred.scores = y_valid_pred_prob.reshape(-1, 1)\n",
    "dataset_orig_test_pred.scores = y_test_pred_prob.reshape(-1, 1)\n",
    "\n",
    "y_train_pred = np.zeros_like(dataset_orig_train_pred.labels)\n",
    "y_train_pred[y_train_pred_prob >= class_thresh] = dataset_orig_train_pred.favorable_label\n",
    "y_train_pred[~(y_train_pred_prob >= class_thresh)] = dataset_orig_train_pred.unfavorable_label\n",
    "dataset_orig_train_pred.labels = y_train_pred\n",
    "\n",
    "y_valid_pred = np.zeros_like(dataset_orig_valid_pred.labels)\n",
    "y_valid_pred[y_valid_pred_prob >= class_thresh] = dataset_orig_valid_pred.favorable_label\n",
    "y_valid_pred[~(y_valid_pred_prob >= class_thresh)] = dataset_orig_valid_pred.unfavorable_label\n",
    "dataset_orig_valid_pred.labels = y_valid_pred\n",
    "\n",
    "y_test_pred = np.zeros_like(dataset_orig_test_pred.labels)\n",
    "y_test_pred[y_test_pred_prob >= class_thresh] = dataset_orig_test_pred.favorable_label\n",
    "y_test_pred[~(y_test_pred_prob >= class_thresh)] = dataset_orig_test_pred.unfavorable_label\n",
    "dataset_orig_test_pred.labels = y_test_pred\n",
    "\n",
    "# Calculate fairness metrics\n",
    "cm_pred_train = ClassificationMetric(dataset_orig_train, dataset_orig_train_pred,\n",
    "                                     unprivileged_groups=unprivileged_groups,\n",
    "                                     privileged_groups=privileged_groups)\n",
    "print(\"#### Original-Predicted training dataset\")\n",
    "print(\"Difference in GFPR between unprivileged and privileged groups\")\n",
    "print(cm_pred_train.difference(cm_pred_train.generalized_false_positive_rate))\n",
    "print(\"Difference in GFNR between unprivileged and privileged groups\")\n",
    "print(cm_pred_train.difference(cm_pred_train.generalized_false_negative_rate))\n",
    "\n",
    "cm_pred_valid = ClassificationMetric(dataset_orig_valid, dataset_orig_valid_pred,\n",
    "                                     unprivileged_groups=unprivileged_groups,\n",
    "                                     privileged_groups=privileged_groups)\n",
    "print(\"#### Original-Predicted validation dataset\")\n",
    "print(\"Difference in GFPR between unprivileged and privileged groups\")\n",
    "print(cm_pred_valid.difference(cm_pred_valid.generalized_false_positive_rate))\n",
    "print(\"Difference in GFNR between unprivileged and privileged groups\")\n",
    "print(cm_pred_valid.difference(cm_pred_valid.generalized_false_negative_rate))\n",
    "\n",
    "cm_pred_test = ClassificationMetric(dataset_orig_test, dataset_orig_test_pred,\n",
    "                                    unprivileged_groups=unprivileged_groups,\n",
    "                                    privileged_groups=privileged_groups)\n",
    "print(\"#### Original-Predicted testing dataset\")\n",
    "print(\"Difference in GFPR between unprivileged and privileged groups\")\n",
    "print(cm_pred_test.difference(cm_pred_test.generalized_false_positive_rate))\n",
    "print(\"Difference in GFNR between unprivileged and privileged groups\")\n",
    "print(cm_pred_test.difference(cm_pred_test.generalized_false_negative_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "153be12e-3d5a-4cb8-b901-473da3c121fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#original random forest model\n",
    "original_model_rf = RandomForestClassifier(random_state=42)\n",
    "original_model_rf.fit(dataset_orig_train.features, dataset_orig_train.labels.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d7ffb1f-d5df-4c8f-a9a4-8cf82f5a60eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy before: 0.9240506329113924\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "test_pred_rf_before = original_model_rf.predict(dataset_orig_test.features)\n",
    "\n",
    "accuracy_before_rf = accuracy_score(dataset_orig_test.labels, test_pred_rf_before)\n",
    "\n",
    "print(\"Random Forest Accuracy before:\", accuracy_before_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09cf833c-0a74-42f4-8816-c15558e3989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Before this cell, are all models before reweighing\n",
    "## After this cell, will be the reweighing process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f93c42e-804e-48c3-9596-59df7963b9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reweighing Process.\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "\n",
    "reweighing = Reweighing(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "dataset_orig_train_rw = reweighing.fit_transform(dataset_orig_train)\n",
    "\n",
    "weights = dataset_orig_train_rw.instance_weights\n",
    "#weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb5504bb-e14f-4288-ae87-11d3dfbbfb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest after reweighing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "510ae0ff-84eb-47d2-987b-50175dba2d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_orig_train.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a97eeb8-6fd4-4f49-8e3b-2856edd952e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_after_rf = RandomForestClassifier(random_state=42)\n",
    "model_after_rf.fit(dataset_orig_train_rw.features, dataset_orig_train_rw.labels.ravel(), sample_weight=dataset_orig_train_rw.instance_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c214af88-c40a-40cf-9a99-8678343a8b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9240506329113924\n",
      "Mean difference (testing set): 0.07727272727272727\n"
     ]
    }
   ],
   "source": [
    "test_pred_rf_after = model_after_rf.predict(dataset_orig_test.features)\n",
    "\n",
    "accuracy_rf_after = accuracy_score(dataset_orig_test.labels, test_pred_rf_after)\n",
    "print(\"Accuracy:\", accuracy_rf_after)\n",
    "\n",
    "testing_metrics = BinaryLabelDatasetMetric(dataset_orig_test, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "print(\"Mean difference (testing set):\", testing_metrics.mean_difference())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bbb93bb-5f52-4e9b-a39f-740aa56f423a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique weights after reweighing: [0.96027935 0.96455696 1.03715802 1.0443038 ]\n"
     ]
    }
   ],
   "source": [
    "# Print unique weights to check their variation after reweighing\n",
    "unique_weights = np.unique(dataset_orig_train_rw.instance_weights)\n",
    "print(\"Unique weights after reweighing:\", unique_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20a85711-fa17-4a36-badd-88865a29c4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution in original training data:\n",
      "1.0    0.535865\n",
      "0.0    0.464135\n",
      "Name: proportion, dtype: float64\n",
      "Weight distribution in reweighted training data:\n",
      "Unique weights: [0.96027935 0.96455696 1.03715802 1.0443038 ]\n",
      "Mean of weights: 0.9999999999999998\n",
      "Standard Deviation of weights: 0.039019276973595474\n",
      "Label distribution in reweighted training data:\n",
      "1.0    0.535865\n",
      "0.0    0.464135\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check the balance of labels and the distribution of weights before and after reweighing\n",
    "print(\"Label distribution in original training data:\")\n",
    "print(pd.Series(dataset_orig_train.labels.ravel()).value_counts(normalize=True))\n",
    "\n",
    "print(\"Weight distribution in reweighted training data:\")\n",
    "unique_weights = np.unique(dataset_orig_train_rw.instance_weights)\n",
    "print(\"Unique weights:\", unique_weights)\n",
    "\n",
    "print(\"Mean of weights:\", np.mean(dataset_orig_train_rw.instance_weights))\n",
    "print(\"Standard Deviation of weights:\", np.std(dataset_orig_train_rw.instance_weights))\n",
    "\n",
    "# Check the distribution of labels in the reweighted data\n",
    "print(\"Label distribution in reweighted training data:\")\n",
    "print(pd.Series(dataset_orig_train_rw.labels.ravel()).value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e722a9b1-85c4-480f-88c6-9b982468e1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth: 3, Min Samples Leaf: 1, Accuracy: 0.9240506329113924\n",
      "Depth: 3, Min Samples Leaf: 5, Accuracy: 0.9240506329113924\n",
      "Depth: 3, Min Samples Leaf: 10, Accuracy: 0.9240506329113924\n",
      "Depth: 5, Min Samples Leaf: 1, Accuracy: 0.9240506329113924\n",
      "Depth: 5, Min Samples Leaf: 5, Accuracy: 0.9240506329113924\n",
      "Depth: 5, Min Samples Leaf: 10, Accuracy: 0.9367088607594937\n",
      "Depth: 10, Min Samples Leaf: 1, Accuracy: 0.9240506329113924\n",
      "Depth: 10, Min Samples Leaf: 5, Accuracy: 0.9240506329113924\n",
      "Depth: 10, Min Samples Leaf: 10, Accuracy: 0.9240506329113924\n",
      "Depth: None, Min Samples Leaf: 1, Accuracy: 0.9240506329113924\n",
      "Depth: None, Min Samples Leaf: 5, Accuracy: 0.9240506329113924\n",
      "Depth: None, Min Samples Leaf: 10, Accuracy: 0.9240506329113924\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Experiment with different model parameters\n",
    "for depth in [3, 5, 10, None]:  # None means no limit on the depth\n",
    "    for min_samples in [1, 5, 10]:\n",
    "        rf_model = RandomForestClassifier(n_estimators=100, max_depth=depth, min_samples_leaf=min_samples, random_state=42)\n",
    "        rf_model.fit(dataset_orig_train_rw.features, dataset_orig_train_rw.labels.ravel(), sample_weight=dataset_orig_train_rw.instance_weights)\n",
    "\n",
    "        # Predicting on test set\n",
    "        test_pred_rf = rf_model.predict(dataset_orig_test.features)\n",
    "        accuracy_rf = accuracy_score(dataset_orig_test.labels, test_pred_rf)\n",
    "\n",
    "        print(f\"Depth: {depth}, Min Samples Leaf: {min_samples}, Accuracy: {accuracy_rf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2af3a4f0-2a2a-4908-a7a5-59010608f68c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "StructuredDataset.__init__() got an unexpected keyword argument 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m X_train_fold, X_test_fold \u001b[38;5;241m=\u001b[39m dataset_orig\u001b[38;5;241m.\u001b[39mfeatures[train_index], dataset_orig\u001b[38;5;241m.\u001b[39mfeatures[test_index]\n\u001b[1;32m     11\u001b[0m y_train_fold, y_test_fold \u001b[38;5;241m=\u001b[39m dataset_orig\u001b[38;5;241m.\u001b[39mlabels\u001b[38;5;241m.\u001b[39mravel()[train_index], dataset_orig\u001b[38;5;241m.\u001b[39mlabels\u001b[38;5;241m.\u001b[39mravel()[test_index]\n\u001b[0;32m---> 12\u001b[0m w_train_fold \u001b[38;5;241m=\u001b[39m reweighing\u001b[38;5;241m.\u001b[39mfit_transform(BinaryLabelDataset(df\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mDataFrame(X_train_fold, columns\u001b[38;5;241m=\u001b[39mdataset_orig\u001b[38;5;241m.\u001b[39mfeature_names),\n\u001b[1;32m     13\u001b[0m                                                            labels\u001b[38;5;241m=\u001b[39my_train_fold))\u001b[38;5;241m.\u001b[39minstance_weights\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Fairness/lib/python3.11/site-packages/aif360/datasets/binary_label_dataset.py:21\u001b[0m, in \u001b[0;36mBinaryLabelDataset.__init__\u001b[0;34m(self, favorable_label, unfavorable_label, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfavorable_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(favorable_label)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munfavorable_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(unfavorable_label)\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28msuper\u001b[39m(BinaryLabelDataset, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: StructuredDataset.__init__() got an unexpected keyword argument 'labels'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Stratified K-Fold to maintain ratio of labels across folds\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = []\n",
    "\n",
    "for train_index, test_index in skf.split(dataset_orig.features, dataset_orig.labels.ravel()):\n",
    "    # Splitting data\n",
    "    X_train_fold, X_test_fold = dataset_orig.features[train_index], dataset_orig.features[test_index]\n",
    "    y_train_fold, y_test_fold = dataset_orig.labels.ravel()[train_index], dataset_orig.labels.ravel()[test_index]\n",
    "    w_train_fold = reweighing.fit_transform(BinaryLabelDataset(df=pd.DataFrame(X_train_fold, columns=dataset_orig.feature_names),\n",
    "                                                               labels=y_train_fold)).instance_weights\n",
    "\n",
    "    # Train model\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train_fold, y_train_fold, sample_weight=w_train_fold)\n",
    "\n",
    "    # Evaluate model\n",
    "    y_pred_fold = model.predict(X_test_fold)\n",
    "    scores.append(accuracy_score(y_test_fold, y_pred_fold))\n",
    "\n",
    "print(\"Cross-validated accuracy scores:\", scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f953b0-6b97-435d-ac69-ad5c0f78d7d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
